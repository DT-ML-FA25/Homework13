{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework13\n",
    "\n",
    "Audio and Neural Networks\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Get more familiar with neural network setup, design, data preparation and training\n",
    "- Practice setting up the objects and parameters for the training loop\n",
    "- Experiment with a pre-trained image neural networks for an audio classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/audio_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/image_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/nn_utils.py\n",
    "\n",
    "!wget -qO- https://github.com/PSAM-5020-2025F-A/5020-utils/releases/latest/download/birds.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Audio\n",
    "from os import listdir, path\n",
    "\n",
    "from torch import nn, Tensor, float32 as t_float32, uint8 as t_uint8\n",
    "from torch.optim import SGD\n",
    "\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from audio_utils import get_samples_and_rate, stft\n",
    "from data_utils import classification_error, display_confusion_matrix\n",
    "from image_utils import make_image\n",
    "from nn_utils import get_labels, get_num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "We're going to see how to train a CNN to classify bird sounds.\n",
    "\n",
    "Yep. These networks are so good at analyzing images that they have also been used to classify audio and text.\n",
    "\n",
    "This usually requires some kind of pre-transformation in order to turn non-visual data into images.\n",
    "\n",
    "This is where we have to get creative, since images are these two-dimensional objects that don't really depend on time, while audio and text are mostly one-dimensional entities with a beginning and an end (we read left-to-right, top-to-bottom, and audio is air pressure change over time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Audio\n",
    "\n",
    "There is a well-defined way to represent audio as a two-dimensional measure using the [Short-Time Fourier Transform](https://en.wikipedia.org/wiki/Short-time_Fourier_transform). We briefly saw this in the very end of our notebook for [Week 04](https://github.com/PSAM-5020-2025F-A/WK04).\n",
    "\n",
    "The Short-Time Fourier Transform can be used to calculate which frequencies are present in subsections (windows) of our audio files. If our audio is $5$ seconds long, we can split it into $200$ subsections of $25$ milliseconds each and compute what frequencies are present in each of these windows.\n",
    "\n",
    "This gives us a two-dimensional representation of our audio, where one direction represents time, the other represents frequency, and the numbers represent how much of a given frequency is present at that time.\n",
    "\n",
    "We have a `stft()` function in our `audio_utils` to simplify this process. It takes a list of audio samples, the sampling rate and the length of the sub-sectioning window to use for the frequency analysis, and returns three lists:\n",
    "\n",
    "- `freqs`: the list of frequency values reported by the analysis. The first element gives us the lowest frequency detected and the last element the highest.\n",
    "- `times`: the time stamps for the windows used in the analysis. If our original file has $80\\text{,}000$ samples and we divide this into windows of $400$ samples, this `times` list will have $200$ values.\n",
    "- `ffts`: this is a list-of-lists, where each row corresponds to a frequency analyzed and each column represents a moment in time. This list should have as many rows as `len(freqs)` and as many columns as `len(times)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio file location\n",
    "filepath = \"./data/audio/birds/train/duck_0001.wav\"\n",
    "\n",
    "# Widget to play the audio\n",
    "display(Audio(filepath))\n",
    "\n",
    "# Read audio file and get its samples and sampling rate\n",
    "samples,rate = get_samples_and_rate(filepath)\n",
    "\n",
    "# Perform the short-time frequency transformation\n",
    "ffts, freqs, times = stft(samples, rate, window_len=400)\n",
    "\n",
    "# Plot results. Times in the x-axis, frequencies in the y-axis,\n",
    "# and the values in ffts represent the strength of a particular frequency at a particular time\n",
    "plt.pcolormesh(times, freqs, ffts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bird Sound Dataset: Labels\n",
    "\n",
    "We have a dataset with $3\\text{,}150$ different audio files of bird sounds. That's $150$ files for each of the $21$ different types of birds in the dataset. The dataset has already been split into `train` and `test` directories, using a $66\\%$ $/$ $33\\%$ split, so twice as many files in the `train` dataset.\n",
    "\n",
    "All we have in the `./data/audio/birds/train` and `./data/audio/birds/test` directories are the audio files. Luckily their names are easy to parse and split into the correct bird label.\n",
    "\n",
    "Write a function that extracts the `birdname` from file path in the form `./data/audio/birds/train/birdname_nnnn.wav`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: write a filepath_to_label() function that turns file paths into bird names\n",
    "# HINT: the split() function can be used twice to separate the file path by '/' and then '_'\n",
    "\n",
    "\n",
    "def filepath_to_label(filepath: str) -> str:\n",
    "    # Split path to get filename\n",
    "    filename = filepath.split('/')[-1]\n",
    "    \n",
    "    # Split filename to get bird name\n",
    "    label = filename.split('_')[0]\n",
    "    \n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bird Sound Dataset: Images\n",
    "\n",
    "This is a bit more complex.\n",
    "\n",
    "We do have the `stft()` function in `audio_utils`, but it returns lists formatted in a way that is easy to visualize the results using `matplotlib`.\n",
    "\n",
    "What we want is a flat list of grayscale pixel values that we can then put in a tensor and pre-process for a CNN.\n",
    "\n",
    "What we have to do is flatten the resulting list-of-lists into a single list of values and then scale the values so they are all between $0$ and $255$.\n",
    "\n",
    "This is basically `MinMaxScaling()`, but we're doing it by hand here so that it runs a little bit faster once we use it on our $3\\text{,}150$ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for turning an audio file into a list of grayscale \"pixels\"\n",
    "\n",
    "def wav_to_pxs(filepath, window_len=400):\n",
    "  samples,rate = get_samples_and_rate(filepath)\n",
    "  ffts = np.array(stft(samples, rate, window_len)[0])\n",
    "  ffts_min, ffts_max = ffts.min(), ffts.max()\n",
    "  pxs = 255 * (ffts - ffts_min) / (ffts_max - ffts_min)\n",
    "  return pxs.reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image from audio\n",
    "\n",
    "We can check that the above function works by testing it on a single file.\n",
    "\n",
    "We just have to call `wav_to_pxs()` with a file path and a length for the sub-sectioning windowing.\n",
    "\n",
    "The result is a list of grayscale pixel values.\n",
    "\n",
    "One thing that helps here is that all files in our dataset already have the same length and sampling rate. This means that the resulting list of pixels will have the same length as well, as long as we use a consistent `window_len` value for all of them.\n",
    "\n",
    "We can recover the \"`height`\" of our images by dividing the `window_len` by $2$. This has to do with how the `stft()` (and its internal `fft()` function) works and how audio with more samples give more granular frequency resolution; more elements in the `freqs` list, more rows in our `ffts` variable.\n",
    "\n",
    "Larger `window_len` values will also mean fewer time steps analyzed (fewer values in the `times` list). So, our `stft()` function always returns the same number of pixels, we just have to decide how we distribute those pixels in the $x$ and $y$ directions using the `window_len` parameter.\n",
    "\n",
    "For our particular dataset, a `window_len` of $400$ gives us square images, but experiment with the `window_len` parameter to see how it affects the resulting image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio file location\n",
    "filepath = \"./data/audio/birds/train/duck_0001.wav\"\n",
    "\n",
    "# TODO: experiment with this value and see how it changes the image\n",
    "window_len = 200\n",
    "\n",
    "# Get pixels from the stft() function\n",
    "pxs = wav_to_pxs(filepath, window_len)\n",
    "\n",
    "# This is how we recover the number of rows and columns in our \"image\"\n",
    "ih = window_len // 2\n",
    "iw = len(pxs) // ih\n",
    "\n",
    "# Check image\n",
    "display(make_image(pxs, iw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "So... What happens ?<br>\n",
    "What are the effects of changing the <code>window_len</code> parameter ?<br>\n",
    "Do we get more or less information about our audio?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">\n",
    "\n",
    "Changing window_len does not add or remove information from the audio. It only changes how the information is represented.\n",
    "\n",
    "Smaller window_len: more time detail, less frequency detail\n",
    "\n",
    "Larger window_len: more frequency detail, less time detail\n",
    "\n",
    "So the audio content stays the same, but the image looks different, which can affect how well the neural network learns patterns.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "\n",
    "Now's the time to go through all of the audio files in our dataset and extract their \"pixels\" and labels.\n",
    "\n",
    "This is similar to how we processes the files for the security camera and LFW datasets.\n",
    "\n",
    "We'll iterate over all of the files, open them, and put their labels in a list and their \"pixels\" in another list.\n",
    "\n",
    "Use square images. CNNs run more efficiently on square images due to the way the convolution kernels are defined.\n",
    "\n",
    "Also, we're processing $2\\text{,}100$ audio files here... this cell might take up to $5$ minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# This is where the training files are\n",
    "TRAIN_DIR = \"./data/audio/birds/train\"\n",
    "\n",
    "# This is a list of all of the training file names\n",
    "train_fnames = sorted([f for f in listdir(TRAIN_DIR) if f.endswith(\"wav\")])\n",
    "\n",
    "# To store the pixels and labels of each file\n",
    "train_pixels = []\n",
    "train_labels = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: iterate through the files, open them and extract pixels and labels\n",
    "\n",
    "window_len = 400  # square images\n",
    "\n",
    "for fname in train_fnames:\n",
    "    filepath = path.join(TRAIN_DIR, fname)\n",
    "    \n",
    "    # Extract pixels\n",
    "    pxs = wav_to_pxs(filepath, window_len)\n",
    "    train_pixels.append(pxs)\n",
    "    \n",
    "    # Extract label\n",
    "    label = filepath_to_label(filepath)\n",
    "    train_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat for the `test` data\n",
    "\n",
    "Again... we're processing $1\\text{,}050$ audio files here... this cell might take up to $3$ minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# This is where the test files are\n",
    "TEST_DIR = \"./data/audio/birds/test\"\n",
    "\n",
    "# This is a list of all of the test file names\n",
    "test_fnames = sorted([f for f in listdir(TEST_DIR) if f.endswith(\"wav\")])\n",
    "\n",
    "# To store the pixels and labels of each file\n",
    "test_pixels = []\n",
    "test_labels = []\n",
    "\n",
    "# TODO: iterate through the files, open them and extract pixels and labels\n",
    "\n",
    "window_len = 400  # must match training data\n",
    "\n",
    "for fname in test_fnames:\n",
    "    filepath = path.join(TEST_DIR, fname)\n",
    "    \n",
    "    # Extract pixels\n",
    "    pxs = wav_to_pxs(filepath, window_len)\n",
    "    test_pixels.append(pxs)\n",
    "    \n",
    "    # Extract label\n",
    "    label = filepath_to_label(filepath)\n",
    "    test_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Everything\n",
    "\n",
    "Now we put everything into `Tensor` objects for our CNN.\n",
    "\n",
    "<!-- <img src=\"./imgs/cnn_layers.jpg\" height=\"200px\" /> -->\n",
    "<!-- <img src=\"./imgs/cnn_fc.jpg\" height=\"200px\" /> -->\n",
    "\n",
    "<img src=\"https://i.postimg.cc/rpdq7DSd/cnn-layers.jpg\" height=\"200px\" />\n",
    "<img src=\"https://i.postimg.cc/XYYjcHqk/cnn-fc.jpg\" height=\"200px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Tensors\n",
    "\n",
    "We need to encode out text labels into numbers before we put them into `Tensors`.\n",
    "\n",
    "We can certainly use an `OrdinalEncoder` from `sklearn`, but a quicker way is to just get a list of all of the unique values in the `train_labels` or `test_labels` lists. Then, we can encode a label text `label_str` by just using the lists `index()` function, like this:\n",
    "\n",
    "`label_int = unique_labels.index(label_str)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Tensors\n",
    "\n",
    "Since CNNs work with $2D$ image information, we have to reshape our pixels back into two-dimensional lists-of-lists. CNNs also expect our images to be represented as single layers, so different images of single-channel pixels and not a single list of multi-channel pixels.\n",
    "\n",
    "Our images are black and white here, so this isn't a big deal, but we still have to use the [`movedim()`](https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim) or [`permute()`](https://pytorch.org/docs/stable/generated/torch.permute.html) functions to re-arrange our `Tensor` dimensions and make it match what is expected by the CNN.\n",
    "\n",
    "If this doesn't sound familiar, take a look at the \"**New Network, New Shape**\" section of our [WK13](https://github.com/PSAM-5020-2025F-A/WK13) notebook for some guidance.\n",
    "\n",
    "Remember that the `height` for our images here is `window_len` divided by $2$, and their `width` is just the number of pixels divided by their `height`. If using a `window_len` of $400$, the images should be squares of $200$ $\\times$ $200$ pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO create a label encoder\n",
    "unique_labels = sorted(list(set(train_labels)))\n",
    "\n",
    "# TODO encode labels and put them into a Tensor of type long() (whole numbers)\n",
    "train_y = Tensor([unique_labels.index(lbl) for lbl in train_labels]).long()\n",
    "test_y  = Tensor([unique_labels.index(lbl) for lbl in test_labels]).long()\n",
    "\n",
    "# TODO encode pixels and put them into a Tensor with shape N x C x H x W\n",
    "window_len = 400\n",
    "H = window_len // 2\n",
    "W = len(train_pixels[0]) // H\n",
    "\n",
    "# Convert train pixels\n",
    "train_X = Tensor(train_pixels).reshape(len(train_pixels), H, W)\n",
    "train_X = train_X.unsqueeze(1)  # add channel dimension (C = 1)\n",
    "\n",
    "# Convert test pixels\n",
    "test_X = Tensor(test_pixels).reshape(len(test_pixels), H, W)\n",
    "test_X = test_X.unsqueeze(1)  # add channel dimension (C = 1)\n",
    "\n",
    "# TODO check shapes\n",
    "print(\"Train X:\", train_X.shape)\n",
    "print(\"Train y:\", train_y.shape)\n",
    "print(\"Test X:\", test_X.shape)\n",
    "print(\"Test y:\", test_y.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_pixels_t = train_X\n",
    "test_pixels_t  = test_X\n",
    "\n",
    "train_labels_t = train_y\n",
    "test_labels_t  = test_y\n",
    "\n",
    "print(\"train_pixels_t:\", train_pixels_t.shape)\n",
    "print(\"train_labels_t:\", train_labels_t.shape)\n",
    "print(\"test_pixels_t:\", test_pixels_t.shape)\n",
    "print(\"test_labels_t:\", test_labels_t.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for ResNet\n",
    "\n",
    "Our data is ready for generic, untrained, plain CNNs, but since we're going to use a pre-trained [Residual Network](https://arxiv.org/abs/1512.03385), we still have a couple of steps left before we're ready to train our model.\n",
    "\n",
    "<!-- <img src=\"./imgs/resnet34_01.jpg\" height=\"200px\" /> -->\n",
    "<img src=\"https://i.postimg.cc/hP20Rn9D/resnet34-01.jpg\" height=\"200px\" />\n",
    "\n",
    "We're going to use the `ReNet34` [pre-trained model](https://pytorch.org/hub/pytorch_vision_resnet/) available in the `PyTorch` library. This is not the largest `ResNet` model, but will fit nicely into small GPUs.\n",
    "\n",
    "The `ResNet` models in `PyTorch` were all trained on the [ImageNet](https://image-net.org/download.php) dataset. This dataset has $1\\text{,}281\\text{,}167$ training images and classifies objects into $1\\text{,}000$ classes.\n",
    "\n",
    "As such, we have to further process our data in order to represent it using the exact same format that was used in the initial training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [documentation](https://pytorch.org/hub/pytorch_vision_resnet/):\n",
    "\n",
    "_All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]._\n",
    "\n",
    "We can use the following  `PyTorch` transformation functions to achieve this.\n",
    "\n",
    "- `ToDtype(t_uint8)`: makes sure our pixels are represented as whole numbers between $0$ and $255$.\n",
    "- `Resize(224)`: makes the images at least $224$ pixels on each side.\n",
    "- `Grayscale(3)`: turns the image into grayscale, but keeps $3$ channels.\n",
    "- `ToDtype(t_float32, scale=True)`: transforms $[0, 255]$ values (`int`) into $[0, 1]$ (`float` ).\n",
    "- `Normalize()`: same as `StandardScaler`, but using pre-fitted values that were derived from the ImageNet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_transforms = v2.Compose([\n",
    "  v2.ToDtype(t_uint8),\n",
    "  v2.Resize(224),\n",
    "  v2.Grayscale(3),\n",
    "  v2.ToDtype(t_float32, scale=True),\n",
    "  v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Transformation\n",
    "\n",
    "We can run the transformation on a single image and use the `ToPILImage()` function to inspect the transformed image. The `ToPILImage()` function undoes the channel transposition and transforms a `PyTorch` image ($3$ separate single-channel images) back into a `PIL` image (single image with $3$ channels).\n",
    "\n",
    "The transformed images will look noisy and should have splotchy colors. This is due to normalizing each of the three channels using different mean and standard deviation values.\n",
    "\n",
    "It doesn't look good for us, but it's what the `ResNet` expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = res_transforms(train_pixels_t[:1])\n",
    "display(v2.ToPILImage()(img_t[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and put on GPU\n",
    "\n",
    "The following cell runs the transformation on the pixel `Tensor` objects and puts them, along with the label `Tensor` objects, on the GPU.\n",
    "\n",
    "This assumes that the labels are in `Tensor` objects called `train_labels_t` and `test_labels_t`, and pixels in `train_pixels_t` and `test_pixels_t`.\n",
    "\n",
    "Adjust if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "x_train_res = res_transforms(train_pixels_t).to(device)\n",
    "x_test_res  = res_transforms(test_pixels_t).to(device)\n",
    "\n",
    "y_train = train_labels_t.to(device)\n",
    "y_test  = test_labels_t.to(device)\n",
    "\n",
    "print(\"Training dataset shape:\", x_train_res.shape)\n",
    "print(\"Image shape:\", x_train_res[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "These are the `PyTorch` objects that we need in order to train our model:\n",
    "\n",
    "- `model`: our network, this will be an instance of a `ResNet34` model\n",
    "- `optim`: optimizer to adjust our model parameters\n",
    "- `loss_fn`: for classification models we can use the `CrossEntropyLoss` function to compute the error of our predictions during training\n",
    "\n",
    "### Modify `ResNet34`\n",
    "\n",
    "The last layer of the default `ResNet34` model has $1\\text{,}000$ outputs, one for each of the classes in the ImageNet dataset, but we're not using that dataset.\n",
    "\n",
    "Since we're classifying our audios into one of $21$ classes, we have to swap the last layer of the `ResNet` model for one with $21$ outputs. The [WK13](https://github.com/PSAM-5020-2025F-A/WK13) notebook has an example of how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO instantiate a resnet34 network\n",
    "model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "\n",
    "# TODO change the network's last layer to match the number of classes\n",
    "num_classes = len(unique_labels)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# TODO put model on device (CPU here)\n",
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# TODO check number of parameters that will be trained\n",
    "print(\"Trainable parameters:\", get_num_params(model))\n",
    "\n",
    "# TODO instantiate an optimizer\n",
    "optim = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# TODO instantiate a loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Network Shapes\n",
    "\n",
    "Run the following cell to make sure the network is connected properly.\n",
    "\n",
    "We're not able to give all of the images to our network at once, so we're using the `batch_step` variable to grab subsets of $15$ images at a time.\n",
    "\n",
    "Our input should have a shape of $15$ x $3$ x $200$ x $200$, representing the number of images, number of channels, height and width of the images in our batch. Likewise, the output should be of shape $15$ x $21$, representing the $21$ class activation values for each of the $15$ images in our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_step = int(len(x_train_res) // 15)\n",
    "\n",
    "out = model(x_train_res[::batch_step])\n",
    "\n",
    "print(\"Input shape:\", x_train_res.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Parameters:\", get_num_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Create a training loop like we saw in [class](https://github.com/PSAM-5020-2025F-A/WK13).\n",
    "\n",
    "Since we can't put all of our images into the model at once, we'll train our model using batches of $15$ images. The `batch_step` variable should already be set by running the cell above, and we can use it like this to get batches inside our main training loop:\n",
    "\n",
    "```python\n",
    "for batch_start in range(batch_step):\n",
    "  batch_input = all_inputs[batch_start::batch_step]\n",
    "  batch_output = all_outputs[batch_start::batch_step]\n",
    "```\n",
    "\n",
    "For each batch in our loop, we should:\n",
    "- Clear the optimizer annotations\n",
    "- Predict classes by feeding inputs into the `model`\n",
    "- Measure `loss` (this is just `loss_fn(predicted, actual)`)\n",
    "- Get the optimizer to back-propagate and annotate the neurons\n",
    "- Update parameters\n",
    "\n",
    "In order to check if the model is overfitting, we can sporadically run evaluations within the training loop in order to see if the model performs similarly with `train` and `test` data.\n",
    "\n",
    "We can use the `get_labels(model, inputs)` function inside the `nn_utils` file to run our `model` on all of the data in a given batch and return the predicted labels for all of the samples. This takes care of evaluating which neuron in our last layer is the most activated.\n",
    "\n",
    "It shouldn't take many epochs to get the training loss close to zero, but even so, running through all $2\\text{,}100$ training images might take about $20$ seconds per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: iterate over epochs\n",
    "# TODO: iterate over batches\n",
    "# TODO: clear optimizer annotations\n",
    "# TODO: get predictions\n",
    "# TODO: compute loss\n",
    "# TODO: annotate neurons with loss values\n",
    "# TODO: adjust model parameters\n",
    "# TODO: every once in a while, print loss and classification error values\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "epochs = 3        # fewer epochs\n",
    "batch_step = 20   # smaller batches\n",
    "\n",
    "# use small subsets for evaluation\n",
    "eval_train = x_train_res[:200]\n",
    "eval_train_y = y_train[:200]\n",
    "\n",
    "eval_test = x_test_res[:200]\n",
    "eval_test_y = y_test[:200]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_start in range(batch_step):\n",
    "        batch_x = x_train_res[batch_start::batch_step]\n",
    "        batch_y = y_train[batch_start::batch_step]\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        preds = model(batch_x)\n",
    "        loss = loss_fn(preds, batch_y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_preds = get_labels(model, eval_train)\n",
    "        test_preds  = get_labels(model, eval_test)\n",
    "    \n",
    "    train_err = classification_error(train_preds, eval_train_y)\n",
    "    test_err  = classification_error(test_preds, eval_test_y)\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch} | \"\n",
    "        f\"Loss: {loss.item():.4f} | \"\n",
    "        f\"Train Error: {train_err:.3f} | \"\n",
    "        f\"Test Error: {test_err:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Now that the model has been trained for our specific bird sound classification task, we can finally calculate some accuracy statistics for it.\n",
    "\n",
    "The `get_labels(model, inputs)` function can be useful here in order to run the model on all of the samples in our datasets and get final predicted label values from the model's final activation layer. These are the numbers we have to use when computing accuracy (maybe precision and recall) and displaying confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Use manageable subsets to avoid kernel crash\n",
    "eval_train = x_train_res[:300]\n",
    "eval_train_y = y_train[:300]\n",
    "\n",
    "eval_test = x_test_res[:300]\n",
    "eval_test_y = y_test[:300]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Predictions\n",
    "    train_preds = get_labels(model, eval_train)\n",
    "    test_preds  = get_labels(model, eval_test)\n",
    "    \n",
    "    # Loss\n",
    "    train_logits = model(eval_train)\n",
    "    test_logits  = model(eval_test)\n",
    "    \n",
    "    train_loss = loss_fn(train_logits, eval_train_y).item()\n",
    "    test_loss  = loss_fn(test_logits, eval_test_y).item()\n",
    "\n",
    "# Classification error\n",
    "train_err = classification_error(train_preds, eval_train_y)\n",
    "test_err  = classification_error(test_preds, eval_test_y)\n",
    "\n",
    "# TODO: print final loss and classification error values\n",
    "print(\"Final Training Loss:\", round(train_loss, 4))\n",
    "print(\"Final Training Error:\", round(train_err, 3))\n",
    "print(\"Final Test Loss:\", round(test_loss, 4))\n",
    "print(\"Final Test Error:\", round(test_err, 3))\n",
    "\n",
    "# TODO: display confusion matrices for train and test data\n",
    "display_confusion_matrix(train_preds, eval_train_y, unique_labels)\n",
    "display_confusion_matrix(test_preds, eval_test_y, unique_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
